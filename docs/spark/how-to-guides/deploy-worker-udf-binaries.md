---
title: Apache Spark 작업자 및 사용자 정의 함수 이진 파일용 .NET 배포
description: Apache Spark 작업자 및 사용자 정의 함수 이진 파일용 .NET을 배포하는 방법에 대해 알아봅니다.
ms.date: 10/09/2020
ms.topic: conceptual
ms.custom: mvc,how-to
ms.openlocfilehash: c777fdb26045c62317b49259fdde974f43ba5c0d
ms.sourcegitcommit: bc293b14af795e0e999e3304dd40c0222cf2ffe4
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 11/26/2020
ms.locfileid: "96293769"
---
# <a name="deploy-net-for-apache-spark-worker-and-user-defined-function-binaries"></a><span data-ttu-id="76efa-103">Apache Spark 작업자 및 사용자 정의 함수 이진 파일용 .NET 배포</span><span class="sxs-lookup"><span data-stu-id="76efa-103">Deploy .NET for Apache Spark worker and user-defined function binaries</span></span>

<span data-ttu-id="76efa-104">이 방법은 Apache Spark 작업자 및 사용자 정의 함수 이진 파일용 .NET을 배포하는 방법에 대한 일반적인 지침을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-104">This how-to provides general instructions on how to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span> <span data-ttu-id="76efa-105">`spark-submit`를 사용하여 애플리케이션을 시작하는 데 일반적으로 사용되는 매개 변수뿐만 아니라 설정할 환경 변수를 알아봅니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-105">You learn which Environment Variables to set up, as well as some commonly used parameters for launching applications with `spark-submit`.</span></span>

## <a name="configurations"></a><span data-ttu-id="76efa-106">구성</span><span class="sxs-lookup"><span data-stu-id="76efa-106">Configurations</span></span>

<span data-ttu-id="76efa-107">구성에는 Apache Spark 작업자 및 사용자 정의 함수 이진 파일용 .NET을 배포하기 위해 일반적인 환경 변수 및 매개 변수 설정이 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-107">Configurations show the general environment variables and parameters settings in order to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span>

### <a name="environment-variables"></a><span data-ttu-id="76efa-108">환경 변수</span><span class="sxs-lookup"><span data-stu-id="76efa-108">Environment variables</span></span>

<span data-ttu-id="76efa-109">작업자를 배포하고 UDF를 작성하는 경우 다음과 같은 일반적으로 사용되는 몇 가지 환경 변수를 설정해야 할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-109">When deploying workers and writing UDFs, there are a few commonly used environment variables that you may need to set:</span></span>

| <span data-ttu-id="76efa-110">환경 변수</span><span class="sxs-lookup"><span data-stu-id="76efa-110">Environment Variable</span></span>         | <span data-ttu-id="76efa-111">설명</span><span class="sxs-lookup"><span data-stu-id="76efa-111">Description</span></span>
| :--------------------------- | :----------
| <span data-ttu-id="76efa-112">DOTNET_WORKER_DIR</span><span class="sxs-lookup"><span data-stu-id="76efa-112">DOTNET_WORKER_DIR</span></span>            | <span data-ttu-id="76efa-113"><code>Microsoft.Spark.Worker</code> 이진 파일이 생성된 경로입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-113">Path where the <code>Microsoft.Spark.Worker</code> binary has been generated.</span></span></br><span data-ttu-id="76efa-114">Spark 드라이버에서 사용되며 Spark 실행기에 전달됩니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-114">It's used by the Spark driver and will be passed to Spark executors.</span></span> <span data-ttu-id="76efa-115">이 변수가 설정되지 않은 경우 Spark 실행기는 <code>PATH</code> 환경 변수에 지정된 경로를 검색합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-115">If this variable is not set up, the Spark executors will search the path specified in the <code>PATH</code> environment variable.</span></span></br><span data-ttu-id="76efa-116">_예: "C:\bin\Microsoft.Spark.Worker"_</span><span class="sxs-lookup"><span data-stu-id="76efa-116">_e.g. "C:\bin\Microsoft.Spark.Worker"_</span></span>
| <span data-ttu-id="76efa-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span><span class="sxs-lookup"><span data-stu-id="76efa-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span></span> | <span data-ttu-id="76efa-118"><code>Microsoft.Spark.Worker</code>에서 어셈블리를 로드할 쉼표로 구분된 경로입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-118">Comma-separated paths where <code>Microsoft.Spark.Worker</code> will load assemblies.</span></span></br><span data-ttu-id="76efa-119">경로가 "."으로 시작하면 작업 디렉터리가 앞에 배치됩니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-119">Note that if a path starts with ".", the working directory will be prepended.</span></span> <span data-ttu-id="76efa-120">**Yarn 모드** 인 경우 "."은 컨테이너의 작업 디렉터리를 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-120">If in **yarn mode**, "." would represent the container's working directory.</span></span></br><span data-ttu-id="76efa-121">_예: "C:\Users\\&lt;사용자 이름&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet 버전&gt;"_</span><span class="sxs-lookup"><span data-stu-id="76efa-121">_e.g. "C:\Users\\&lt;user name&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet version&gt;"_</span></span>
| <span data-ttu-id="76efa-122">DOTNET_WORKER_DEBUG</span><span class="sxs-lookup"><span data-stu-id="76efa-122">DOTNET_WORKER_DEBUG</span></span>          | <span data-ttu-id="76efa-123"><a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">UDF를 디버그</a>하려면 <code>spark-submit</code>를 실행하기 전에 이 환경 변수를 <code>1</code>로 설정합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-123">If you want to <a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">debug a UDF</a>, then set this environment variable to <code>1</code> before running <code>spark-submit</code>.</span></span>

### <a name="parameter-options"></a><span data-ttu-id="76efa-124">매개 변수 옵션</span><span class="sxs-lookup"><span data-stu-id="76efa-124">Parameter options</span></span>

<span data-ttu-id="76efa-125">Spark 애플리케이션이 [번들로 제공](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies)되면 `spark-submit`을 사용하여 시작할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-125">Once the Spark application is [bundled](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies), you can launch it using `spark-submit`.</span></span> <span data-ttu-id="76efa-126">다음 표에서는 일반적으로 사용되는 일부 옵션을 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-126">The following table shows some of the commonly used options:</span></span>

| <span data-ttu-id="76efa-127">매개 변수 이름</span><span class="sxs-lookup"><span data-stu-id="76efa-127">Parameter Name</span></span>        | <span data-ttu-id="76efa-128">설명</span><span class="sxs-lookup"><span data-stu-id="76efa-128">Description</span></span>
| :---------------------| :----------
| <span data-ttu-id="76efa-129">--class</span><span class="sxs-lookup"><span data-stu-id="76efa-129">--class</span></span>               | <span data-ttu-id="76efa-130">애플리케이션의 진입점입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-130">The entry point for your application.</span></span></br><span data-ttu-id="76efa-131">_예: org.apache.spark.deploy.dotnet.DotnetRunner_</span><span class="sxs-lookup"><span data-stu-id="76efa-131">_e.g. org.apache.spark.deploy.dotnet.DotnetRunner_</span></span>
| <span data-ttu-id="76efa-132">--master</span><span class="sxs-lookup"><span data-stu-id="76efa-132">--master</span></span>              | <span data-ttu-id="76efa-133">클러스터의 <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">마스터 URL</a>입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-133">The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">master URL</a> for the cluster.</span></span></br><span data-ttu-id="76efa-134">_예: Yarn_</span><span class="sxs-lookup"><span data-stu-id="76efa-134">_e.g. yarn_</span></span>
| <span data-ttu-id="76efa-135">--deploy-mode</span><span class="sxs-lookup"><span data-stu-id="76efa-135">--deploy-mode</span></span>         | <span data-ttu-id="76efa-136">작업자 노드(<code>cluster</code>) 또는 외부 클라이언트(<code>client</code>)로 로컬 배포할지 여부입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-136">Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>).</span></span></br><span data-ttu-id="76efa-137">기본값: <code>client</code></span><span class="sxs-lookup"><span data-stu-id="76efa-137">Default: <code>client</code></span></span>
| <span data-ttu-id="76efa-138">--conf</span><span class="sxs-lookup"><span data-stu-id="76efa-138">--conf</span></span>                | <span data-ttu-id="76efa-139"><code>key=value</code> 형식의 임의 Spark 구성 속성입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-139">Arbitrary Spark configuration property in <code>key=value</code> format.</span></span></br><span data-ttu-id="76efa-140">_예: spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker_</span><span class="sxs-lookup"><span data-stu-id="76efa-140">_e.g. spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker_</span></span>
| <span data-ttu-id="76efa-141">--files</span><span class="sxs-lookup"><span data-stu-id="76efa-141">--files</span></span>               | <span data-ttu-id="76efa-142">각 실행기의 작업 디렉터리에 배치될 파일의 쉼표로 구분된 목록입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-142">Comma-separated list of files to be placed in the working directory of each executor.</span></span><br/><ul><li><span data-ttu-id="76efa-143">이 옵션은 Yarn 모드에만 적용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-143">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="76efa-144">Hadoop과 비슷한 #으로 파일 이름 지정을 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-144">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="76efa-145">_예: <code>myLocalSparkApp.dll#appSeen.dll</code> 애플리케이션은 YARN에서 실행하는 경우 <code>myLocalSparkApp.dll</code>를 참조하려면 이름을 <code>appSeen.dll</code>로 사용해야 합니다._</span><span class="sxs-lookup"><span data-stu-id="76efa-145">_e.g. <code>myLocalSparkApp.dll#appSeen.dll</code>. Your application should use the name as <code>appSeen.dll</code> to reference <code>myLocalSparkApp.dll</code> when running on YARN._</span></span></li>
| <span data-ttu-id="76efa-146">--archives</span><span class="sxs-lookup"><span data-stu-id="76efa-146">--archives</span></span>          | <span data-ttu-id="76efa-147">각 실행기의 작업 디렉터리로 추출될 보관의 쉼표로 구분된 목록입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-147">Comma-separated list of archives to be extracted into the working directory of each executor.</span></span></br><ul><li><span data-ttu-id="76efa-148">이 옵션은 Yarn 모드에만 적용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-148">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="76efa-149">Hadoop과 비슷한 #으로 파일 이름 지정을 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-149">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="76efa-150">_예: <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code> 그러면 zip 파일이 <code>worker</code> 폴더에 복사되고 압축을 풉니다._</span><span class="sxs-lookup"><span data-stu-id="76efa-150">_e.g. <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>. This will copy and extract the zip file to <code>worker</code> folder._</span></span></li>
| <span data-ttu-id="76efa-151">application-jar</span><span class="sxs-lookup"><span data-stu-id="76efa-151">application-jar</span></span>       | <span data-ttu-id="76efa-152">애플리케이션 및 모든 종속성을 포함하여 번들로 제공되는 Jar의 경로입니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-152">Path to a bundled jar including your application and all dependencies.</span></span></br><span data-ttu-id="76efa-153">_Dp: hdfs://&lt;Jar 경로&gt;/microsoft-spark-&lt;version&gt;.jar_</span><span class="sxs-lookup"><span data-stu-id="76efa-153">_e.g. hdfs://&lt;path to your jar&gt;/microsoft-spark-&lt;version&gt;.jar_</span></span>
| <span data-ttu-id="76efa-154">application-arguments</span><span class="sxs-lookup"><span data-stu-id="76efa-154">application-arguments</span></span> | <span data-ttu-id="76efa-155">주 클래스의 주 메서드에 전달된 인수입니다(있는 경우).</span><span class="sxs-lookup"><span data-stu-id="76efa-155">Arguments passed to the main method of your main class, if any.</span></span></br><span data-ttu-id="76efa-156">_예: hdfs://&lt;앱 경로&gt;/&lt;사용 중인 앱&gt;.zip &lt;앱 이름&gt; &lt;앱 인수&gt;_</span><span class="sxs-lookup"><span data-stu-id="76efa-156">_e.g. hdfs://&lt;path to your app&gt;/&lt;your app&gt;.zip &lt;your app name&gt; &lt;app args&gt;_</span></span>

> [!NOTE]
> <span data-ttu-id="76efa-157">`spark-submit`를 사용하여 애플리케이션을 시작하는 경우 `application-jar` 전에 모든 `--options`을 지정합니다. 지정하지 않으면 무시됩니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-157">Specify all the `--options` before `application-jar` when launching applications with `spark-submit`, otherwise they will be ignored.</span></span> <span data-ttu-id="76efa-158">자세한 내용은 [`spark-submit` 옵션](https://spark.apache.org/docs/latest/submitting-applications.html) 및 [YARN에서 Spark 실행 세부 정보](https://spark.apache.org/docs/latest/running-on-yarn.html)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="76efa-158">For more information, see [`spark-submit` options](https://spark.apache.org/docs/latest/submitting-applications.html) and [running spark on YARN details](https://spark.apache.org/docs/latest/running-on-yarn.html).</span></span>

## <a name="frequently-asked-questions"></a><span data-ttu-id="76efa-159">질문과 대답</span><span class="sxs-lookup"><span data-stu-id="76efa-159">Frequently asked questions</span></span>

### <a name="when-i-run-a-spark-app-with-udfs-i-get-a-filenotfoundexception-error-what-should-i-do"></a><span data-ttu-id="76efa-160">UDF를 사용하여 Spark 앱을 실행하면 \`FileNotFoundException' 오류가 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-160">When I run a spark app with UDFs, I get a \`FileNotFoundException' error.</span></span> <span data-ttu-id="76efa-161">어떻게 해야 합니까?</span><span class="sxs-lookup"><span data-stu-id="76efa-161">What should I do?</span></span>

> <span data-ttu-id="76efa-162">**오류:** [오류] [작업 실행자] [0] ProcessStream()이 다음 예외를 나타내며 실패함: System.IO.FileNotFoundException: 어셈블리 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' 파일을 찾을 수 없음: 'mySparkApp.dll'</span><span class="sxs-lookup"><span data-stu-id="76efa-162">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.IO.FileNotFoundException: Assembly 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' file not found: 'mySparkApp.dll'</span></span>

<span data-ttu-id="76efa-163">**대답:** `DOTNET_ASSEMBLY_SEARCH_PATHS` 환경 변수가 올바르게 설정되었는지 확인하세요.</span><span class="sxs-lookup"><span data-stu-id="76efa-163">**Answer:** Check that the `DOTNET_ASSEMBLY_SEARCH_PATHS` environment variable is set correctly.</span></span> <span data-ttu-id="76efa-164">`mySparkApp.dll`이 포함된 경로여야 합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-164">It should be the path that contains your `mySparkApp.dll`.</span></span>

### <a name="after-i-upgraded-my-net-for-apache-spark-version-and-reset-the-dotnet_worker_dir-environment-variable-why-do-i-still-get-the-following-ioexception-error"></a><span data-ttu-id="76efa-165">Apache Spark 버전의 .NET을 업그레이드하고 `DOTNET_WORKER_DIR` 환경 변수를 다시 설정한 후에도 다음 `IOException` 오류가 계속 발생하는 이유는 무엇인가요?</span><span class="sxs-lookup"><span data-stu-id="76efa-165">After I upgraded my .NET for Apache Spark version and reset the `DOTNET_WORKER_DIR` environment variable, why do I still get the following `IOException` error?</span></span>

> <span data-ttu-id="76efa-166">**오류:** 11.0 단계에서 작업 0.0 손실(TID 24, localhost, 실행기 드라이버): java.io.IOException: "Microsoft.Spark.Worker.exe" 프로그램을 실행할 수 없음: CreateProcess error=2, 지정한 파일을 시스템에서 찾을 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-166">**Error:** Lost task 0.0 in stage 11.0 (TID 24, localhost, executor driver): java.io.IOException: Cannot run program "Microsoft.Spark.Worker.exe": CreateProcess error=2, The system cannot find the file specified.</span></span>

<span data-ttu-id="76efa-167">**대답:** 최신 환경 변수 값을 사용할 수 있도록 PowerShell 창(또는 기타 명령 창)을 먼저 다시 시작하세요.</span><span class="sxs-lookup"><span data-stu-id="76efa-167">**Answer:** Try restarting your PowerShell window (or other command windows) first so that it can take the latest environment variable values.</span></span> <span data-ttu-id="76efa-168">그런 다음, 프로그램을 시작합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-168">Then start your program.</span></span>

### <a name="after-submitting-my-spark-application-i-get-the-error-systemtypeloadexception-could-not-load-type-systemruntimeremotingcontextscontext"></a><span data-ttu-id="76efa-169">Spark 애플리케이션을 제출한 후에 `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'` 오류가 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-169">After submitting my Spark application, I get the error `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`.</span></span>

> <span data-ttu-id="76efa-170">**오류:** [오류] [작업 실행자] [0] ProcessStream()이 다음 예외를 나타내며 실패함: System.TypeLoadException: 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...' 어셈블리에서 'System.Runtime.Remoting.Contexts.Context' 형식을 로드할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-170">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context' from assembly 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...'.</span></span>

<span data-ttu-id="76efa-171">**대답:** 사용 중인 `Microsoft.Spark.Worker` 버전을 확인하세요.</span><span class="sxs-lookup"><span data-stu-id="76efa-171">**Answer:** Check the `Microsoft.Spark.Worker` version you are using.</span></span> <span data-ttu-id="76efa-172">**.NET Framework 4.6.1** 및 **.NET Core 3.1.x** 의 두 가지 버전이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-172">There are two versions: **.NET Framework 4.6.1** and **.NET Core 3.1.x**.</span></span> <span data-ttu-id="76efa-173">이 경우 `System.Runtime.Remoting.Contexts.Context`는 .NET Framework 전용이기 때문에 `Microsoft.Spark.Worker.net461.win-x64-<version>`([다운로드](https://github.com/dotnet/spark/releases)할 수 있음)을 사용해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-173">In this case, `Microsoft.Spark.Worker.net461.win-x64-<version>` (which you can [download](https://github.com/dotnet/spark/releases)) should be used since `System.Runtime.Remoting.Contexts.Context` is only for .NET Framework.</span></span>

### <a name="how-do-i-run-my-spark-application-with-udfs-on-yarn-which-environment-variables-and-parameters-should-i-use"></a><span data-ttu-id="76efa-174">YARN에서 UDF를 사용하여 내 Spark 애플리케이션을 실행하는 방법은 무엇인가요?</span><span class="sxs-lookup"><span data-stu-id="76efa-174">How do I run my spark application with UDFs on YARN?</span></span> <span data-ttu-id="76efa-175">어떤 환경 변수와 매개 변수를 사용해야 하나요?</span><span class="sxs-lookup"><span data-stu-id="76efa-175">Which environment variables and parameters should I use?</span></span>

<span data-ttu-id="76efa-176">**대답:** YARN에서 Spark 애플리케이션을 시작하려면 환경 변수를 `spark.yarn.appMasterEnv.[EnvironmentVariableName]`로 지정해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="76efa-176">**Answer:** To launch the spark application on YARN, the environment variables should be specified as `spark.yarn.appMasterEnv.[EnvironmentVariableName]`.</span></span> <span data-ttu-id="76efa-177">`spark-submit`을 사용하는 예제는 아래를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="76efa-177">Please see below as an example using `spark-submit`:</span></span>

```powershell
spark-submit \
--class org.apache.spark.deploy.dotnet.DotnetRunner \
--master yarn \
--deploy-mode cluster \
--conf spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=./worker/Microsoft.Spark.Worker-<version> \
--conf spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS=./udfs \
--archives hdfs://<path to your files>/Microsoft.Spark.Worker.net461.win-x64-<version>.zip#worker,hdfs://<path to your files>/mySparkApp.zip#udfs \
hdfs://<path to jar file>/microsoft-spark-<spark_majorversion-spark_minorversion>_<scala_majorversion.scala_minorversion>-<spark_dotnet_version>.jar \
hdfs://<path to your files>/mySparkApp.zip mySparkApp
```

## <a name="next-steps"></a><span data-ttu-id="76efa-178">다음 단계</span><span class="sxs-lookup"><span data-stu-id="76efa-178">Next steps</span></span>

* [<span data-ttu-id="76efa-179">.NET for Apache Spark 시작</span><span class="sxs-lookup"><span data-stu-id="76efa-179">Get started with .NET for Apache Spark</span></span>](../tutorials/get-started.md)
* [<span data-ttu-id="76efa-180">Windows에서 .NET for Apache Spark 애플리케이션 디버그</span><span class="sxs-lookup"><span data-stu-id="76efa-180">Debug a .NET for Apache Spark application on Windows</span></span>](debug.md)
